
import numpy as np
from pathlib import Path

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    roc_auc_score,
    f1_score,
    precision_score,
    recall_score,
    classification_report,
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

import joblib

from .preprocessing import (
    load_telco_data,
    basic_cleaning,
    encode_target,
    add_feature_engineering,
    build_preprocess_transformer,
)

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = PROJECT_ROOT / "data" / "Telco-Customer-Churn.csv"
MODELS_DIR = PROJECT_ROOT / "models"
MODELS_DIR.mkdir(exist_ok=True)


def get_models():
    return {
        "log_reg": LogisticRegression(max_iter=1000, n_jobs=-1),
        "rf": RandomForestClassifier(
            n_estimators=300, max_depth=None, random_state=42, n_jobs=-1
        ),
        "xgb": XGBClassifier(
            n_estimators=300,
            max_depth=4,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            objective="binary:logistic",
            eval_metric="logloss",
            n_jobs=-1,
            random_state=42,
        ),
        "lgbm": LGBMClassifier(
            n_estimators=300,
            learning_rate=0.05,
            num_leaves=31,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
        ),
        "catboost": CatBoostClassifier(
            iterations=300,
            depth=6,
            learning_rate=0.05,
            loss_function="Logloss",
            verbose=0,
            random_state=42,
        ),
    }


def train_and_select_model():
    print(f"Loading data from: {DATA_PATH}")
    df = load_telco_data(str(DATA_PATH))
    df = basic_cleaning(df)
    X, y = encode_target(df)

    # Feature engineering
    X = add_feature_engineering(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Build preprocessor on training data
    preprocessor = build_preprocess_transformer(X_train)
    models = get_models()

    scoring = {
        "roc_auc": "roc_auc",
        "f1": "f1",
        "precision": "precision",
        "recall": "recall",
    }
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    best_model_name = None
    best_roc_auc = -np.inf
    best_estimator = None

    for name, model in models.items():
        pipe = Pipeline(
            steps=[
                ("preprocess", preprocessor),
                ("model", model),
            ]
        )
        scores = cross_validate(
            pipe,
            X_train,
            y_train,
            cv=cv,
            scoring=scoring,
            n_jobs=-1,
            return_estimator=True,
        )

        mean_roc = scores["test_roc_auc"].mean()
        std_roc = scores["test_roc_auc"].std()
        mean_f1 = scores["test_f1"].mean()

        print(f"Model: {name}")
        print(
            f"  ROC-AUC: {mean_roc:.3f} Â± {std_roc:.3f} | "
            f"F1: {mean_f1:.3f}"
        )
        print("-" * 60)

        if mean_roc > best_roc_auc:
            best_roc_auc = mean_roc
            best_model_name = name
            # best estimator index
            best_idx = np.argmax(scores["test_roc_auc"])
            best_estimator = scores["estimator"][best_idx]

    print(f"\nBest model by CV ROC-AUC: {best_model_name} ({best_roc_auc:.3f})")

    # Evaluate on test set
    y_proba = best_estimator.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= 0.5).astype(int)

    print("\n=== Test Performance ===")
    print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}")
    print(f"F1: {f1_score(y_test, y_pred):.3f}")
    print(f"Precision: {precision_score(y_test, y_pred):.3f}")
    print(f"Recall: {recall_score(y_test, y_pred):.3f}")
    print("\nClassification report:")
    print(classification_report(y_test, y_pred))

    # Save best pipeline
    model_path = MODELS_DIR / "best_churn_model.joblib"
    joblib.dump(best_estimator, model_path)
    print(f"\nSaved best model pipeline to: {model_path}")

    return best_estimator


if __name__ == "__main__":
    train_and_select_model()
